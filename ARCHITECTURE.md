# Multi-Agent Reasoning Architecture for `rlm-repo-intel`

This system treats the RLM REPL itself as the multi-agent runtime.

The root model writes Python orchestration code. Each `sub_rlm(...)` call is one agent role with its own system prompt and context window. Agent outputs are persisted as REPL variables, so downstream agents can inspect and challenge prior opinions.

## Core Model

- Root model role: `Synthesizer`
- Worker roles via `sub_rlm(...)`:
  - `Code Analyst`
  - `Codebase Expert`
  - `Risk Assessor`
  - `Adversarial Reviewer`
- Shared memory: Python variables in REPL (workspace dict)

## Agent Roles in RLM

1. `Code Analyst`
- Reads raw PR diff + metadata.
- Produces technical change summary, changed components, preliminary quality/risk/test scores.

2. `Codebase Expert`
- Reads module cards + architecture map + analyst output.
- Evaluates architecture fit, strategic value, and novelty.

3. `Risk Assessor`
- Reads analyst + expert outputs.
- Focuses on breakage risk, test coverage adequacy, and security concerns.

4. `Adversarial Reviewer`
- Reads all prior outputs.
- Explicitly tries to reject the PR and challenge optimistic scores.

5. `Synthesizer` (root)
- Reads all agent outputs + disagreement points.
- Resolves conflicts with explicit rationale.
- Emits final scores and decision explanation.

## How Agents Check Each Other

- Every agent output is stored as a named REPL variable.
- Later agents reference those variables directly.
- The adversarial reviewer reads prior scores and writes counter-arguments.
- The root synthesizer produces explicit conflict resolution notes.

Example variable chain:
- `code_analyst_output` -> consumed by `codebase_expert_output`
- `code_analyst_output` + `codebase_expert_output` -> consumed by `risk_assessor_output`
- All three -> challenged by `adversarial_output`
- All four + disagreement list -> resolved by `synthesized_output`

## REPL as Shared Workspace

The root model-generated Python inside the REPL looks like this:

```python
# Root orchestrator code generated by the root RLM.
workspace = {}

workspace["pr_input"] = {
    "number": pr_number,
    "title": pr_title,
    "body": pr_body,
    "diff": pr_diff,
    "metadata": pr_metadata,
}
workspace["module_cards"] = relevant_module_cards
workspace["architecture"] = architecture_snapshot

code_analyst_output = sub_rlm(
    system_prompt="""
    You are Code Analyst. Analyze the diff technically.
    Return JSON: technical_summary, changed_components,
    quality_score, risk_score, test_alignment, confidence, reasoning.
    """,
    context={
        "pr_input": workspace["pr_input"],
        "module_cards": workspace["module_cards"],
    },
)
workspace["code_analyst_output"] = code_analyst_output

codebase_expert_output = sub_rlm(
    system_prompt="""
    You are Codebase Expert. Validate architecture fit.
    Return JSON: architecture_fit, strategic_value, novelty_score,
    concern_points, confidence, reasoning.
    """,
    context={
        "pr_input": workspace["pr_input"],
        "module_cards": workspace["module_cards"],
        "architecture": workspace["architecture"],
        "code_analyst_output": workspace["code_analyst_output"],
    },
)
workspace["codebase_expert_output"] = codebase_expert_output

risk_assessor_output = sub_rlm(
    system_prompt="""
    You are Risk Assessor. Find breakage/security/testing risks.
    Return JSON: risk_score, test_alignment, security_risk,
    high_risk_items, confidence, reasoning.
    """,
    context={
        "pr_input": workspace["pr_input"],
        "code_analyst_output": workspace["code_analyst_output"],
        "codebase_expert_output": workspace["codebase_expert_output"],
    },
)
workspace["risk_assessor_output"] = risk_assessor_output

adversarial_output = sub_rlm(
    system_prompt="""
    You are Adversarial Reviewer. Try to reject this PR.
    Challenge prior scores and assumptions.
    Return JSON: reject_reasons, challenged_scores,
    rejection_confidence, counter_arguments, confidence, reasoning.
    """,
    context={
        "pr_input": workspace["pr_input"],
        "code_analyst_output": workspace["code_analyst_output"],
        "codebase_expert_output": workspace["codebase_expert_output"],
        "risk_assessor_output": workspace["risk_assessor_output"],
    },
)
workspace["adversarial_output"] = adversarial_output

workspace["disagreement_points"] = []
if abs(code_analyst_output["risk_score"] - risk_assessor_output["risk_score"]) >= 0.25:
    workspace["disagreement_points"].append("Risk scores diverge materially.")
if codebase_expert_output["novelty_score"] >= 0.7 and adversarial_output["rejection_confidence"] >= 0.6:
    workspace["disagreement_points"].append("High novelty claim challenged by rejection argument.")
if adversarial_output.get("challenged_scores"):
    workspace["disagreement_points"].append("Adversarial reviewer challenged prior scoring.")

synthesized_output = sub_rlm(
    system_prompt="""
    You are Synthesizer (root). Resolve disagreements explicitly.
    Return JSON: risk_score, quality_score, strategic_value,
    novelty_score, test_alignment, review_summary,
    conflict_candidates, redundancy_candidates,
    synthesis_reasoning, confidence.
    """,
    context={
        "pr_input": workspace["pr_input"],
        "code_analyst_output": workspace["code_analyst_output"],
        "codebase_expert_output": workspace["codebase_expert_output"],
        "risk_assessor_output": workspace["risk_assessor_output"],
        "adversarial_output": workspace["adversarial_output"],
        "disagreement_points": workspace["disagreement_points"],
    },
)
workspace["synthesized_output"] = synthesized_output
```

## Single-PR Decision Flow (End-to-End)

1. Load PR diff + metadata into `workspace["pr_input"]`.
2. Load relevant module cards into `workspace["module_cards"]`.
3. Call `code_analyst` sub-agent and store `workspace["code_analyst_output"]`.
4. Call `codebase_expert` with analyst output and store `workspace["codebase_expert_output"]`.
5. Call `risk_assessor` with prior outputs and store `workspace["risk_assessor_output"]`.
6. Call `adversarial_reviewer` with all previous outputs and store `workspace["adversarial_output"]`.
7. Root synthesizer resolves disagreements and stores `workspace["synthesized_output"]`.
8. Persist all outputs + disagreement log into evaluation artifacts for traceability.

## Cross-PR Debate (Deduplication and Conflict Resolution)

For PR pair or group comparison, use debate variables in shared REPL:

```python
pair_workspace = {
    "pr_a": pr_eval_a,
    "pr_b": pr_eval_b,
}

pair_workspace["proposal"] = sub_rlm(
    system_prompt="""
    You are Cross-PR Proposer.
    Propose relation: redundant, alternative, conflicting, composable, unrelated.
    """,
    context=pair_workspace,
)

pair_workspace["challenge"] = sub_rlm(
    system_prompt="""
    You are Cross-PR Adversarial Reviewer.
    Challenge the proposal and provide a counter-relation.
    """,
    context={
        "pr_a": pair_workspace["pr_a"],
        "pr_b": pair_workspace["pr_b"],
        "proposal": pair_workspace["proposal"],
    },
)

pair_workspace["resolution"] = sub_rlm(
    system_prompt="""
    You are Cross-PR Synthesizer.
    Resolve proposer vs challenger with explicit rationale.
    """,
    context=pair_workspace,
)
```

Example argument pattern:
- Proposer: "PRs #4517/#4521 are redundant (same module + same issue)."
- Challenger: "#4521 uses a different architecture boundary and is alternative, not redundant."
- Root resolution: keep `alternative` with explicit justification and confidence.

## Implemented Storage Model

The code now persists agent reasoning in artifacts:

- PR evaluation traces: `.rlm-repo-intel/results/pr_reasoning_traces.jsonl`
- PR evaluations (includes agent outputs): `.rlm-repo-intel/results/pr_evaluations.jsonl`
- Cross-PR debate logs: `.rlm-repo-intel/results/pr_relation_debates.jsonl`
- Pair relations (includes proposal/challenge/resolution): `.rlm-repo-intel/results/pr_relations.jsonl`

This provides auditability for why each PR scored as it did, and why cross-PR relations were accepted or rejected.
